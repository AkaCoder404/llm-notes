Running on node:
octave
CUDA version:
/var/spool/slurm/d/job123468/slurm_script: line 24: nvcc: command not found
Python version:
Python 3.11.5
nvidia_modeset       1310720  0
nvidia_uvm           1536000  0
nvidia              56705024  2 nvidia_uvm,nvidia_modeset
video                  65536  1 nvidia_modeset
drm                   614400  7 drm_kms_helper,drm_vram_helper,ast,nvidia,drm_ttm_helper,ttm
Thu May 16 15:57:22 2024       
+---------------------------------------------------------------------------------------+
| NVIDIA-SMI 535.129.03             Driver Version: 535.129.03   CUDA Version: 12.2     |
|-----------------------------------------+----------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |
|                                         |                      |               MIG M. |
|=========================================+======================+======================|
|   0  NVIDIA A100-PCIE-40GB          On  | 00000000:61:00.0 Off |                    0 |
| N/A   28C    P0              32W / 250W |      4MiB / 40960MiB |      0%      Default |
|                                         |                      |             Disabled |
+-----------------------------------------+----------------------+----------------------+
                                                                                         
+---------------------------------------------------------------------------------------+
| Processes:                                                                            |
|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |
|        ID   ID                                                             Usage      |
|=======================================================================================|
|  No running processes found                                                           |
+---------------------------------------------------------------------------------------+
2.3.0+cu121
True
打印每个传递的参数：
../hf/OpenBMB/MiniCPM-2B-dpo-fp32
1
1
logs/OpenBMB/MiniCPM-2B-dpo-fp32
gsm8k
gen
7
Available GPUs: 1: 0
The num of datasets is: 61
Datasets for evaluation: gsm8k
The number of selected datasets is 1; the number of selected tasks is 1.
Results have been saved！
./scripts/run_job_base.sh: line 38: --dtype: command not found
[2024-05-16 15:57:30 +0800] [130416] [INFO] Starting gunicorn 22.0.0
[2024-05-16 15:57:30 +0800] [130416] [INFO] gunicorn app, gpus_num=1, workers_num=1, per_worker_gpus=1
[2024-05-16 15:57:30 +0800] [130416] [INFO] Listening at: http://127.0.0.1:5002 (130416)
[2024-05-16 15:57:30 +0800] [130416] [INFO] Using worker: sync
[2024-05-16 15:57:30 +0800] [130417] [INFO] Booting worker with pid: 130417
[2024-05-16 15:57:30 +0800] [130417] [INFO] server.age=1, worker.age=1, worker.pid=130417, gpus=[0]
INFO 05-16 15:57:35 llm_engine.py:100] Initializing an LLM engine (v0.4.2) with config: model='../hf/OpenBMB/MiniCPM-2B-dpo-fp32', speculative_config=None, tokenizer='../hf/OpenBMB/MiniCPM-2B-dpo-fp32', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=../hf/OpenBMB/MiniCPM-2B-dpo-fp32)
INFO 05-16 15:57:36 utils.py:660] Found nccl from library /home/ltq/.config/vllm/nccl/cu12/libnccl.so.2.18.1
INFO 05-16 15:57:39 selector.py:81] Cannot use FlashAttention-2 backend because the flash_attn package is not found. Please install it for better performance.
INFO 05-16 15:57:39 selector.py:32] Using XFormers backend.
INFO 05-16 15:59:17 model_runner.py:175] Loading model weights took 5.1034 GB
INFO 05-16 15:59:19 gpu_executor.py:114] # GPU blocks: 5214, # CPU blocks: 728
INFO 05-16 15:59:21 model_runner.py:937] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 05-16 15:59:21 model_runner.py:941] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 05-16 15:59:26 model_runner.py:1017] Graph capturing finished in 5 secs.
model load finished
127.0.0.1 - - [16/May/2024:15:59:26 +0800] "GET /infer HTTP/1.1" 405 153 "-" "curl/7.88.1"
Service is up!
Postprocessing method: general_torch
Params file: models/model_params/vllm_sample.json
-------final CMD is------
python main.py --model general --model_args url=http://127.0.0.1:5002/infer,concurrency=1 --config_path configs/eval_config.json --output_base_path logs/OpenBMB/MiniCPM-2B-dpo-fp32 --batch_size 32 --postprocess general_torch --params models/model_params/vllm_sample.json --write_out --num_fewshot 7
-------final CMD end------
  0%|          | 0/42 [00:00<?, ?it/s]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:25,  6.62s/it]Processed prompts:   6%|▋         | 2/32 [00:07<01:34,  3.14s/it]Processed prompts:   9%|▉         | 3/32 [00:07<00:51,  1.79s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:34,  1.30s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:27,  1.07s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.25it/s]
127.0.0.1 - - [16/May/2024:15:59:39 +0800] "POST /infer HTTP/1.1" 200 23586 "-" "python-requests/2.31.0"
  2%|▏         | 1/42 [00:09<06:49, 10.00s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:48,  5.45s/it]Processed prompts:   6%|▋         | 2/32 [00:07<01:47,  3.57s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:24,  2.92s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.25it/s]
127.0.0.1 - - [16/May/2024:15:59:48 +0800] "POST /infer HTTP/1.1" 200 24844 "-" "python-requests/2.31.0"
  5%|▍         | 2/42 [00:19<06:38,  9.96s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:05,  4.04s/it]Processed prompts:   6%|▋         | 2/32 [00:09<02:33,  5.10s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.24it/s]
127.0.0.1 - - [16/May/2024:15:59:58 +0800] "POST /infer HTTP/1.1" 200 24973 "-" "python-requests/2.31.0"
  7%|▋         | 3/42 [00:29<06:28,  9.96s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]WARNING 05-16 16:00:00 scheduler.py:648] Input prompt (2095 tokens) is too long and exceeds limit of 2048
Processed prompts:   3%|▎         | 1/32 [00:01<00:33,  1.09s/it]Processed prompts:   6%|▋         | 2/32 [00:07<02:14,  4.47s/it]Processed prompts:   9%|▉         | 3/32 [00:08<01:15,  2.60s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<01:00,  2.16s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]
127.0.0.1 - - [16/May/2024:16:00:08 +0800] "POST /infer HTTP/1.1" 200 25536 "-" "python-requests/2.31.0"
 10%|▉         | 4/42 [00:39<06:16,  9.92s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:51,  5.54s/it]Processed prompts:   6%|▋         | 2/32 [00:07<01:36,  3.21s/it]Processed prompts:   9%|▉         | 3/32 [00:07<00:56,  1.94s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<00:57,  2.04s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.29it/s]
127.0.0.1 - - [16/May/2024:16:00:18 +0800] "POST /infer HTTP/1.1" 200 24828 "-" "python-requests/2.31.0"
 12%|█▏        | 5/42 [00:49<06:05,  9.88s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:08<04:33,  8.83s/it]Processed prompts:   6%|▋         | 2/32 [00:09<01:56,  3.89s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:09,  2.41s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.23it/s]
127.0.0.1 - - [16/May/2024:16:00:28 +0800] "POST /infer HTTP/1.1" 200 25725 "-" "python-requests/2.31.0"
 14%|█▍        | 6/42 [00:59<05:56,  9.91s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:11,  4.25s/it]Processed prompts:   6%|▋         | 2/32 [00:04<01:05,  2.18s/it]Processed prompts:   9%|▉         | 3/32 [00:07<01:07,  2.33s/it]Processed prompts:  12%|█▎        | 4/32 [00:08<00:51,  1.82s/it]Processed prompts:  16%|█▌        | 5/32 [00:08<00:33,  1.24s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:30,  1.16s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]
127.0.0.1 - - [16/May/2024:16:00:38 +0800] "POST /infer HTTP/1.1" 200 24911 "-" "python-requests/2.31.0"
 17%|█▋        | 7/42 [01:09<05:46,  9.89s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:49,  3.55s/it]Processed prompts:   6%|▋         | 2/32 [00:09<02:29,  4.99s/it]Processed prompts:   9%|▉         | 3/32 [00:10<01:26,  2.99s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.15it/s]
127.0.0.1 - - [16/May/2024:16:00:48 +0800] "POST /infer HTTP/1.1" 200 24850 "-" "python-requests/2.31.0"
 19%|█▉        | 8/42 [01:19<05:40, 10.00s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:36,  5.06s/it]Processed prompts:   6%|▋         | 2/32 [00:09<02:26,  4.90s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.25it/s]
127.0.0.1 - - [16/May/2024:16:00:58 +0800] "POST /infer HTTP/1.1" 200 24063 "-" "python-requests/2.31.0"
 21%|██▏       | 9/42 [01:29<05:29,  9.97s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:17,  4.44s/it]Processed prompts:   6%|▋         | 2/32 [00:04<00:58,  1.95s/it]Processed prompts:   9%|▉         | 3/32 [00:07<01:07,  2.34s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<01:02,  2.22s/it]Processed prompts:  16%|█▌        | 5/32 [00:10<00:43,  1.61s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.20it/s]
127.0.0.1 - - [16/May/2024:16:01:08 +0800] "POST /infer HTTP/1.1" 200 24284 "-" "python-requests/2.31.0"
 24%|██▍       | 10/42 [01:39<05:20, 10.01s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:07<03:45,  7.29s/it]Processed prompts:   6%|▋         | 2/32 [00:09<02:14,  4.47s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:13,  2.52s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.21it/s]
127.0.0.1 - - [16/May/2024:16:01:18 +0800] "POST /infer HTTP/1.1" 200 25251 "-" "python-requests/2.31.0"
 26%|██▌       | 11/42 [01:49<05:10, 10.02s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:09<04:39,  9.00s/it]Processed prompts:   6%|▋         | 2/32 [00:09<01:58,  3.96s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:04,  2.24s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<00:41,  1.48s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.22it/s]
127.0.0.1 - - [16/May/2024:16:01:28 +0800] "POST /infer HTTP/1.1" 200 25738 "-" "python-requests/2.31.0"
 29%|██▊       | 12/42 [01:59<05:00, 10.02s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:43,  3.32s/it]Processed prompts:   6%|▋         | 2/32 [00:07<01:56,  3.89s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:24,  2.91s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<00:50,  1.80s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:35,  1.32s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.21it/s]
127.0.0.1 - - [16/May/2024:16:01:38 +0800] "POST /infer HTTP/1.1" 200 24408 "-" "python-requests/2.31.0"
 31%|███       | 13/42 [02:09<04:50, 10.02s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:20,  4.52s/it]Processed prompts:   6%|▋         | 2/32 [00:05<01:08,  2.30s/it]Processed prompts:   9%|▉         | 3/32 [00:08<01:14,  2.55s/it]Processed prompts:  12%|█▎        | 4/32 [00:08<00:52,  1.88s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:41,  1.54s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.23it/s]
127.0.0.1 - - [16/May/2024:16:01:48 +0800] "POST /infer HTTP/1.1" 200 23470 "-" "python-requests/2.31.0"
 33%|███▎      | 14/42 [02:19<04:40, 10.01s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:45,  5.34s/it]Processed prompts:   6%|▋         | 2/32 [00:07<01:43,  3.45s/it]Processed prompts:   9%|▉         | 3/32 [00:07<01:01,  2.11s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<00:48,  1.72s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:37,  1.39s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.22it/s]
127.0.0.1 - - [16/May/2024:16:01:58 +0800] "POST /infer HTTP/1.1" 200 24426 "-" "python-requests/2.31.0"
 36%|███▌      | 15/42 [02:29<04:30, 10.01s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:08<04:11,  8.12s/it]Processed prompts:   6%|▋         | 2/32 [00:09<01:57,  3.93s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:08,  2.37s/it]Processed prompts:  12%|█▎        | 4/32 [00:10<00:45,  1.61s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.17it/s]
127.0.0.1 - - [16/May/2024:16:02:08 +0800] "POST /infer HTTP/1.1" 200 26087 "-" "python-requests/2.31.0"
 38%|███▊      | 16/42 [02:39<04:21, 10.05s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]WARNING 05-16 16:02:09 scheduler.py:648] Input prompt (2072 tokens) is too long and exceeds limit of 2048
Processed prompts:   3%|▎         | 1/32 [00:00<00:20,  1.49it/s]Processed prompts:   6%|▋         | 2/32 [00:05<01:30,  3.00s/it]Processed prompts:   9%|▉         | 3/32 [00:06<01:02,  2.15s/it]Processed prompts:  12%|█▎        | 4/32 [00:07<00:44,  1.61s/it]Processed prompts:  16%|█▌        | 5/32 [00:07<00:30,  1.12s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:37,  1.46s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.34it/s]
127.0.0.1 - - [16/May/2024:16:02:18 +0800] "POST /infer HTTP/1.1" 200 22876 "-" "python-requests/2.31.0"
 40%|████      | 17/42 [02:49<04:08,  9.93s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:24,  4.66s/it]Processed prompts:   6%|▋         | 2/32 [00:06<01:37,  3.26s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:28,  3.04s/it]Processed prompts:  12%|█▎        | 4/32 [00:10<00:55,  1.97s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.19it/s]
127.0.0.1 - - [16/May/2024:16:02:28 +0800] "POST /infer HTTP/1.1" 200 24602 "-" "python-requests/2.31.0"
 43%|████▎     | 18/42 [02:59<03:59,  9.99s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:08<04:15,  8.25s/it]Processed prompts:   6%|▋         | 2/32 [00:09<02:08,  4.29s/it]Processed prompts:   9%|▉         | 3/32 [00:10<01:12,  2.51s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.15it/s]
127.0.0.1 - - [16/May/2024:16:02:38 +0800] "POST /infer HTTP/1.1" 200 24260 "-" "python-requests/2.31.0"
 45%|████▌     | 19/42 [03:09<03:51, 10.06s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:08<04:13,  8.19s/it]Processed prompts:   6%|▋         | 2/32 [00:09<01:56,  3.89s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:09,  2.38s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.31it/s]
127.0.0.1 - - [16/May/2024:16:02:48 +0800] "POST /infer HTTP/1.1" 200 25411 "-" "python-requests/2.31.0"
 48%|████▊     | 20/42 [03:19<03:40, 10.00s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:02<01:19,  2.55s/it]Processed prompts:   6%|▋         | 2/32 [00:05<01:24,  2.82s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:33,  3.21s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<00:59,  2.12s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:38,  1.42s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.24it/s]
127.0.0.1 - - [16/May/2024:16:02:58 +0800] "POST /infer HTTP/1.1" 200 23738 "-" "python-requests/2.31.0"
 50%|█████     | 21/42 [03:29<03:29,  9.99s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:28,  6.72s/it]Processed prompts:   6%|▋         | 2/32 [00:06<01:25,  2.85s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:17,  2.69s/it]Processed prompts:  12%|█▎        | 4/32 [00:10<00:53,  1.92s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.17it/s]
127.0.0.1 - - [16/May/2024:16:03:08 +0800] "POST /infer HTTP/1.1" 200 24447 "-" "python-requests/2.31.0"
 52%|█████▏    | 22/42 [03:39<03:20, 10.04s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]WARNING 05-16 16:03:10 scheduler.py:648] Input prompt (2057 tokens) is too long and exceeds limit of 2048
Processed prompts:   3%|▎         | 1/32 [00:00<00:30,  1.00it/s]Processed prompts:   6%|▋         | 2/32 [00:04<01:19,  2.63s/it]Processed prompts:   9%|▉         | 3/32 [00:05<00:54,  1.89s/it]Processed prompts:  12%|█▎        | 4/32 [00:06<00:45,  1.62s/it]Processed prompts:  16%|█▌        | 5/32 [00:08<00:44,  1.65s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:36,  1.41s/it]Processed prompts:  22%|██▏       | 7/32 [00:09<00:25,  1.03s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.23it/s]
127.0.0.1 - - [16/May/2024:16:03:18 +0800] "POST /infer HTTP/1.1" 200 23498 "-" "python-requests/2.31.0"
 55%|█████▍    | 23/42 [03:49<03:11, 10.06s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:05<03:00,  5.81s/it]Processed prompts:   6%|▋         | 2/32 [00:07<01:34,  3.14s/it]Processed prompts:  12%|█▎        | 4/32 [00:08<00:44,  1.60s/it]Processed prompts:  16%|█▌        | 5/32 [00:08<00:31,  1.16s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:30,  1.18s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.25it/s]
127.0.0.1 - - [16/May/2024:16:03:28 +0800] "POST /infer HTTP/1.1" 200 24842 "-" "python-requests/2.31.0"
 57%|█████▋    | 24/42 [03:59<03:00, 10.02s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:09<05:05,  9.84s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.25it/s]
127.0.0.1 - - [16/May/2024:16:03:38 +0800] "POST /infer HTTP/1.1" 200 25557 "-" "python-requests/2.31.0"
 60%|█████▉    | 25/42 [04:09<02:49,  9.99s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:07<03:50,  7.45s/it]Processed prompts:   6%|▋         | 2/32 [00:08<01:58,  3.94s/it]Processed prompts:   9%|▉         | 3/32 [00:10<01:19,  2.73s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.13it/s]
127.0.0.1 - - [16/May/2024:16:03:49 +0800] "POST /infer HTTP/1.1" 200 24840 "-" "python-requests/2.31.0"
 62%|██████▏   | 26/42 [04:20<02:41, 10.08s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:19,  6.43s/it]Processed prompts:   9%|▉         | 3/32 [00:06<00:49,  1.72s/it]Processed prompts:  12%|█▎        | 4/32 [00:07<00:37,  1.36s/it]Processed prompts:  16%|█▌        | 5/32 [00:08<00:32,  1.19s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:32,  1.26s/it]Processed prompts:  22%|██▏       | 7/32 [00:09<00:24,  1.03it/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.25it/s]
127.0.0.1 - - [16/May/2024:16:03:59 +0800] "POST /infer HTTP/1.1" 200 24183 "-" "python-requests/2.31.0"
 64%|██████▍   | 27/42 [04:29<02:30, 10.03s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:58,  3.82s/it]Processed prompts:   6%|▋         | 2/32 [00:04<00:59,  1.97s/it]Processed prompts:   9%|▉         | 3/32 [00:07<01:11,  2.45s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:42,  1.58s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:32,  1.25s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.27it/s]
127.0.0.1 - - [16/May/2024:16:04:08 +0800] "POST /infer HTTP/1.1" 200 24091 "-" "python-requests/2.31.0"
 67%|██████▋   | 28/42 [04:39<02:19,  9.98s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:11,  4.23s/it]Processed prompts:   6%|▋         | 2/32 [00:06<01:37,  3.25s/it]Processed prompts:  12%|█▎        | 4/32 [00:07<00:38,  1.38s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:46,  1.73s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.25it/s]
127.0.0.1 - - [16/May/2024:16:04:18 +0800] "POST /infer HTTP/1.1" 200 23970 "-" "python-requests/2.31.0"
 69%|██████▉   | 29/42 [04:49<02:09,  9.96s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:10<05:13, 10.10s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.17it/s]
127.0.0.1 - - [16/May/2024:16:04:28 +0800] "POST /infer HTTP/1.1" 200 25545 "-" "python-requests/2.31.0"
 71%|███████▏  | 30/42 [04:59<02:00, 10.03s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:08<04:09,  8.04s/it]Processed prompts:   6%|▋         | 2/32 [00:08<01:41,  3.39s/it]Processed prompts:   9%|▉         | 3/32 [00:08<01:01,  2.14s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<00:47,  1.68s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:30,  1.12s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.22it/s]
127.0.0.1 - - [16/May/2024:16:04:39 +0800] "POST /infer HTTP/1.1" 200 25283 "-" "python-requests/2.31.0"
 74%|███████▍  | 31/42 [05:09<01:50, 10.02s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]WARNING 05-16 16:04:40 scheduler.py:648] Input prompt (2085 tokens) is too long and exceeds limit of 2048
Processed prompts:   3%|▎         | 1/32 [00:01<00:46,  1.50s/it]Processed prompts:   6%|▋         | 2/32 [00:06<01:42,  3.41s/it]Processed prompts:   9%|▉         | 3/32 [00:06<00:58,  2.03s/it]Processed prompts:  12%|█▎        | 4/32 [00:06<00:37,  1.35s/it]Processed prompts:  16%|█▌        | 5/32 [00:08<00:39,  1.48s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:34,  1.31s/it]Processed prompts:  22%|██▏       | 7/32 [00:09<00:24,  1.01it/s]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.21it/s]
127.0.0.1 - - [16/May/2024:16:04:49 +0800] "POST /infer HTTP/1.1" 200 23496 "-" "python-requests/2.31.0"
 76%|███████▌  | 32/42 [05:20<01:40, 10.03s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:30,  4.84s/it]Processed prompts:   6%|▋         | 2/32 [00:06<01:33,  3.12s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:27,  3.02s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.31it/s]
127.0.0.1 - - [16/May/2024:16:04:58 +0800] "POST /infer HTTP/1.1" 200 24834 "-" "python-requests/2.31.0"
 79%|███████▊  | 33/42 [05:29<01:29,  9.94s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:09<04:54,  9.49s/it]Processed prompts:   6%|▋         | 2/32 [00:09<02:05,  4.19s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.21it/s]
127.0.0.1 - - [16/May/2024:16:05:08 +0800] "POST /infer HTTP/1.1" 200 25619 "-" "python-requests/2.31.0"
 81%|████████  | 34/42 [05:39<01:19,  9.97s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:06<03:20,  6.46s/it]Processed prompts:   6%|▋         | 2/32 [00:07<01:36,  3.21s/it]Processed prompts:  12%|█▎        | 4/32 [00:07<00:38,  1.37s/it]Processed prompts:  16%|█▌        | 5/32 [00:08<00:33,  1.22s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:28,  1.11s/it]Processed prompts:  22%|██▏       | 7/32 [00:10<00:21,  1.15it/s]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.20it/s]
127.0.0.1 - - [16/May/2024:16:05:18 +0800] "POST /infer HTTP/1.1" 200 23814 "-" "python-requests/2.31.0"
 83%|████████▎ | 35/42 [05:49<01:10, 10.00s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:29,  4.81s/it]Processed prompts:   6%|▋         | 2/32 [00:08<01:55,  3.87s/it]Processed prompts:   9%|▉         | 3/32 [00:08<01:03,  2.20s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<00:51,  1.85s/it]Processed prompts:  16%|█▌        | 5/32 [00:10<00:38,  1.43s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.13it/s]
127.0.0.1 - - [16/May/2024:16:05:29 +0800] "POST /infer HTTP/1.1" 200 23607 "-" "python-requests/2.31.0"
 86%|████████▌ | 36/42 [06:00<01:00, 10.10s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:56,  5.70s/it]Processed prompts:   6%|▋         | 2/32 [00:08<01:51,  3.71s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:12,  2.51s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<00:47,  1.70s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:31,  1.18s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.26it/s]
127.0.0.1 - - [16/May/2024:16:05:39 +0800] "POST /infer HTTP/1.1" 200 24200 "-" "python-requests/2.31.0"
 88%|████████▊ | 37/42 [06:10<00:50, 10.04s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:03<01:59,  3.85s/it]Processed prompts:   6%|▋         | 2/32 [00:07<01:46,  3.54s/it]Processed prompts:   9%|▉         | 3/32 [00:07<01:06,  2.29s/it]Processed prompts:  12%|█▎        | 4/32 [00:08<00:40,  1.45s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:35,  1.30s/it]Processed prompts:  19%|█▉        | 6/32 [00:09<00:28,  1.11s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.23it/s]
127.0.0.1 - - [16/May/2024:16:05:49 +0800] "POST /infer HTTP/1.1" 200 24494 "-" "python-requests/2.31.0"
 90%|█████████ | 38/42 [06:20<00:40, 10.02s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:04<02:06,  4.09s/it]Processed prompts:   6%|▋         | 2/32 [00:05<01:17,  2.58s/it]Processed prompts:   9%|▉         | 3/32 [00:10<01:39,  3.42s/it]Processed prompts:  12%|█▎        | 4/32 [00:10<00:59,  2.13s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.15it/s]
127.0.0.1 - - [16/May/2024:16:05:59 +0800] "POST /infer HTTP/1.1" 200 24997 "-" "python-requests/2.31.0"
 93%|█████████▎| 39/42 [06:30<00:30, 10.09s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:05<02:42,  5.24s/it]Processed prompts:   6%|▋         | 2/32 [00:07<01:39,  3.31s/it]Processed prompts:   9%|▉         | 3/32 [00:09<01:22,  2.85s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<00:50,  1.80s/it]Processed prompts:  16%|█▌        | 5/32 [00:10<00:35,  1.31s/it]Processed prompts: 100%|██████████| 32/32 [00:10<00:00,  3.16it/s]
127.0.0.1 - - [16/May/2024:16:06:09 +0800] "POST /infer HTTP/1.1" 200 24444 "-" "python-requests/2.31.0"
 95%|█████████▌| 40/42 [06:40<00:20, 10.12s/it]Processed prompts:   0%|          | 0/32 [00:00<?, ?it/s]Processed prompts:   3%|▎         | 1/32 [00:02<01:11,  2.30s/it]Processed prompts:   6%|▋         | 2/32 [00:05<01:22,  2.74s/it]Processed prompts:   9%|▉         | 3/32 [00:07<01:10,  2.42s/it]Processed prompts:  12%|█▎        | 4/32 [00:09<01:04,  2.31s/it]Processed prompts:  16%|█▌        | 5/32 [00:09<00:42,  1.58s/it]Processed prompts: 100%|██████████| 32/32 [00:09<00:00,  3.26it/s]
127.0.0.1 - - [16/May/2024:16:06:19 +0800] "POST /infer HTTP/1.1" 200 24913 "-" "python-requests/2.31.0"
 98%|█████████▊| 41/42 [06:50<00:10, 10.05s/it]Processed prompts:   0%|          | 0/7 [00:00<?, ?it/s]Processed prompts:  14%|█▍        | 1/7 [00:02<00:13,  2.32s/it]Processed prompts:  29%|██▊       | 2/7 [00:04<00:10,  2.08s/it]Processed prompts: 100%|██████████| 7/7 [00:04<00:00,  1.66it/s]
127.0.0.1 - - [16/May/2024:16:06:23 +0800] "POST /infer HTTP/1.1" 200 5367 "-" "python-requests/2.31.0"
100%|██████████| 42/42 [06:54<00:00,  8.31s/it]100%|██████████| 42/42 [06:54<00:00,  9.87s/it]
<<gsm8k_gsm8k_gen>> Gathered metrics are: defaultdict(<class 'list'>, {'accuracy': [1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1]})
<<gsm8k_gsm8k_gen>> Final Metric is: {'accuracy': 0.4495830174374526}
For detailed output of the model, see logs/OpenBMB/MiniCPM-2B-dpo-fp32/2024-05-16_15-59-28/gsm8k_gsm8k_gen/instance.jsonl

Here are the results for each task:
|     Task      | Metric |Value |
|---------------|--------|-----:|
|gsm8k_gsm8k_gen|accuracy|0.4496|


Here are the results for each dataset:
|Dataset| Metric |Value |
|-------|--------|-----:|
|gsm8k  |accuracy|0.4496|

Path 'logs/OpenBMB/MiniCPM-2B-dpo-fp32/2024-05-16_15-59-28' already exists.

The results of all tasks have been saved to the logs/OpenBMB/MiniCPM-2B-dpo-fp32/2024-05-16_15-59-28/_all_results.json

Running time: 415.19761085510254 seconds
Running time: 415.19761085510254 seconds, the whole time: 415.19761657714844 seconds
[2024-05-16 16:06:24 +0800] [130416] [INFO] Handling signal: term
[2024-05-16 16:06:24 +0800] [130417] [INFO] Worker exiting (pid: 130417)
[2024-05-16 16:06:28 +0800] [130416] [WARNING] worker exit. pid=130417, gpus=[0]
[2024-05-16 16:06:28 +0800] [130416] [INFO] Shutting down: Master
